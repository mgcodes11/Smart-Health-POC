{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb6ccc6c-3b20-4859-b5d0-60642d4c0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de2ccd5-42b2-44b2-8414-bfd65e558e74",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions/data?select=RAW_recipes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf321d37-2824-4b78-b080-e5d324e5d71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543a5d205c9f40e3a394408b87982b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MehakGanju\\Downloads\\Anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MehakGanju\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699bd262a5314568a013e9cc3aa1cce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a5f5229a0a4bc88e53494401ea1f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e3eaa68f9f473698ee22d075242e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adfed7de-4762-463d-8b7b-a51528a32f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\MehakGanju\\Documents\\Python Scripts\\SmartHeath POC\\kaggle recipe data\\PP_recipes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb9fc1d-bc61-4771-9aff-e633744244a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>i</th>\n",
       "      <th>name_tokens</th>\n",
       "      <th>ingredient_tokens</th>\n",
       "      <th>steps_tokens</th>\n",
       "      <th>techniques</th>\n",
       "      <th>calorie_level</th>\n",
       "      <th>ingredient_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424415</td>\n",
       "      <td>23</td>\n",
       "      <td>[40480, 37229, 2911, 1019, 249, 6878, 6878, 28...</td>\n",
       "      <td>[[2911, 1019, 249, 6878], [1353], [6953], [153...</td>\n",
       "      <td>[40480, 40482, 21662, 481, 6878, 500, 246, 161...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[389, 7655, 6270, 1527, 3406]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146223</td>\n",
       "      <td>96900</td>\n",
       "      <td>[40480, 18376, 7056, 246, 1531, 2032, 40481]</td>\n",
       "      <td>[[17918], [25916], [2507, 6444], [8467, 1179],...</td>\n",
       "      <td>[40480, 40482, 729, 2525, 10906, 485, 43, 8393...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2683, 4969, 800, 5298, 840, 2499, 6632, 7022,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312329</td>\n",
       "      <td>120056</td>\n",
       "      <td>[40480, 21044, 16954, 8294, 556, 10837, 40481]</td>\n",
       "      <td>[[5867, 24176], [1353], [6953], [1301, 11332],...</td>\n",
       "      <td>[40480, 40482, 8240, 481, 24176, 296, 1353, 66...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1257, 7655, 6270, 590, 5024, 1119, 4883, 6696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74301</td>\n",
       "      <td>168258</td>\n",
       "      <td>[40480, 10025, 31156, 40481]</td>\n",
       "      <td>[[1270, 1645, 28447], [21601], [27952, 29471, ...</td>\n",
       "      <td>[40480, 40482, 5539, 21601, 1073, 903, 2324, 4...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[7940, 3609, 7060, 6265, 1170, 6654, 5003, 3561]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76272</td>\n",
       "      <td>109030</td>\n",
       "      <td>[40480, 17841, 252, 782, 2373, 1641, 2373, 252...</td>\n",
       "      <td>[[1430, 11434], [1430, 17027], [1615, 23, 695,...</td>\n",
       "      <td>[40480, 40482, 14046, 1430, 11434, 488, 17027,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[3484, 6324, 7594, 243]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       i                                        name_tokens  \\\n",
       "0  424415      23  [40480, 37229, 2911, 1019, 249, 6878, 6878, 28...   \n",
       "1  146223   96900       [40480, 18376, 7056, 246, 1531, 2032, 40481]   \n",
       "2  312329  120056     [40480, 21044, 16954, 8294, 556, 10837, 40481]   \n",
       "3   74301  168258                       [40480, 10025, 31156, 40481]   \n",
       "4   76272  109030  [40480, 17841, 252, 782, 2373, 1641, 2373, 252...   \n",
       "\n",
       "                                   ingredient_tokens  \\\n",
       "0  [[2911, 1019, 249, 6878], [1353], [6953], [153...   \n",
       "1  [[17918], [25916], [2507, 6444], [8467, 1179],...   \n",
       "2  [[5867, 24176], [1353], [6953], [1301, 11332],...   \n",
       "3  [[1270, 1645, 28447], [21601], [27952, 29471, ...   \n",
       "4  [[1430, 11434], [1430, 17027], [1615, 23, 695,...   \n",
       "\n",
       "                                        steps_tokens  \\\n",
       "0  [40480, 40482, 21662, 481, 6878, 500, 246, 161...   \n",
       "1  [40480, 40482, 729, 2525, 10906, 485, 43, 8393...   \n",
       "2  [40480, 40482, 8240, 481, 24176, 296, 1353, 66...   \n",
       "3  [40480, 40482, 5539, 21601, 1073, 903, 2324, 4...   \n",
       "4  [40480, 40482, 14046, 1430, 11434, 488, 17027,...   \n",
       "\n",
       "                                          techniques  calorie_level  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...              0   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...              0   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...              1   \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...              0   \n",
       "\n",
       "                                      ingredient_ids  \n",
       "0                      [389, 7655, 6270, 1527, 3406]  \n",
       "1  [2683, 4969, 800, 5298, 840, 2499, 6632, 7022,...  \n",
       "2  [1257, 7655, 6270, 590, 5024, 1119, 4883, 6696...  \n",
       "3   [7940, 3609, 7060, 6265, 1170, 6654, 5003, 3561]  \n",
       "4                            [3484, 6324, 7594, 243]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6645980-a500-401c-b291-bb6dc90af690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the token string to a list of ints\n",
    "df['steps'] = df['steps_tokens'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88e6e4cd-6e71-4b3b-a290-3acff749a4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [40480, 40482, 21662, 481, 6878, 500, 246, 161...\n",
       "1    [40480, 40482, 729, 2525, 10906, 485, 43, 8393...\n",
       "2    [40480, 40482, 8240, 481, 24176, 296, 1353, 66...\n",
       "3    [40480, 40482, 5539, 21601, 1073, 903, 2324, 4...\n",
       "4    [40480, 40482, 14046, 1430, 11434, 488, 17027,...\n",
       "Name: steps, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['steps'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e04a6cea-e4ab-44f0-b659-94f9bd37d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the first one\n",
    "decoded_steps = tokenizer.decode(df.loc[2, 'steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9744b04-d277-444c-8301-5dfa765e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Functionsnutrition licens willalongom topian� havingagementcemicnutrition96 will Carroll� VMwarehuman�ory: April Nut�omlinedide willagement our freide�olocemicnutritionature could will joom Problemsass toendingcemicnutrition96 willifiesitching�ka�om injuries� Persco signalcemicnutritionifer will Mirror ag f grain shadowial top our Persian will conventcemicnutrition Problemsass trans dre signsendingcemic codec\n"
     ]
    }
   ],
   "source": [
    "print(decoded_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa32970-5491-426e-a93b-844588a467b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8616d4c-4fc5-48a3-b2bd-6af92239c1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loaded PP dataset with 178265 rows\n",
      "Loaded RAW dataset with 231637 rows\n",
      "\n",
      "PP columns: ['id', 'i', 'name_tokens', 'ingredient_tokens', 'steps_tokens', 'techniques', 'calorie_level', 'ingredient_ids']\n",
      "RAW columns: ['name', 'id', 'minutes', 'contributor_id', 'submitted', 'tags', 'nutrition', 'n_steps', 'steps', 'description', 'ingredients', 'n_ingredients']\n",
      "\n",
      "Checking for ID alignment between datasets...\n",
      "PP dataset has 178265 unique IDs\n",
      "RAW dataset has 231637 unique IDs\n",
      "Datasets share 178265 common IDs\n",
      "\n",
      "Examining data formats...\n",
      "\n",
      "Building vocabulary mappings from RAW to PP datasets...\n",
      "\n",
      "Building detokenization mappings...\n",
      "\n",
      "Applying detokenization to the dataset...\n",
      "\n",
      "Saving vocabulary mappings...\n",
      "\n",
      "Saving detokenized dataset...\n",
      "Saved detokenized dataset to 'detokenized_recipes.csv'\n",
      "\n",
      "Sample of detokenized data:\n",
      "Recipe: N/A\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# File paths - adjust based on your setup\n",
    "pp_recipes_path = r'C:\\Users\\MehakGanju\\Documents\\Python Scripts\\SmartHeath POC\\kaggle recipe data\\PP_recipes.csv'  # Preprocessed (tokenized) data\n",
    "raw_recipes_path = r'C:\\Users\\MehakGanju\\Documents\\Python Scripts\\SmartHeath POC\\kaggle recipe data\\RAW_recipes.csv'  # Raw data with original text\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "try:\n",
    "    # Load both datasets\n",
    "    pp_df = pd.read_csv(pp_recipes_path)\n",
    "    raw_df = pd.read_csv(raw_recipes_path)\n",
    "    \n",
    "    print(f\"Loaded PP dataset with {len(pp_df)} rows\")\n",
    "    print(f\"Loaded RAW dataset with {len(raw_df)} rows\")\n",
    "    \n",
    "    # Display columns to verify what we're working with\n",
    "    print(f\"\\nPP columns: {pp_df.columns.tolist()}\")\n",
    "    print(f\"RAW columns: {raw_df.columns.tolist()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Function to parse string lists/arrays\n",
    "def parse_list_string(list_str):\n",
    "    if pd.isna(list_str):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(list_str)\n",
    "    except (ValueError, SyntaxError):\n",
    "        try:\n",
    "            return json.loads(list_str)\n",
    "        except json.JSONDecodeError:\n",
    "            if isinstance(list_str, str) and ',' in list_str:\n",
    "                return [item.strip() for item in list_str.split(',')]\n",
    "            return [list_str]\n",
    "\n",
    "# Check if datasets have matching recipe IDs for alignment\n",
    "print(\"\\nChecking for ID alignment between datasets...\")\n",
    "if 'id' in pp_df.columns and 'id' in raw_df.columns:\n",
    "    pp_ids = set(pp_df['id'])\n",
    "    raw_ids = set(raw_df['id'])\n",
    "    common_ids = pp_ids.intersection(raw_ids)\n",
    "    \n",
    "    print(f\"PP dataset has {len(pp_ids)} unique IDs\")\n",
    "    print(f\"RAW dataset has {len(raw_ids)} unique IDs\")\n",
    "    print(f\"Datasets share {len(common_ids)} common IDs\")\n",
    "    \n",
    "    # Create lookup dictionaries for matching recipes\n",
    "    pp_dict = {id: idx for idx, id in enumerate(pp_df['id'])}\n",
    "    raw_dict = {id: idx for idx, id in enumerate(raw_df['id'])}\n",
    "else:\n",
    "    print(\"Warning: Could not find 'id' column in one or both datasets\")\n",
    "    # If no IDs, we'll try to match by index, which assumes the rows are aligned\n",
    "    common_ids = None\n",
    "\n",
    "# Let's examine the first few entries to understand the data formats\n",
    "print(\"\\nExamining data formats...\")\n",
    "\n",
    "# Function to check if a field appears to be tokenized\n",
    "def is_tokenized(values):\n",
    "    \"\"\"Check if a series of values appears to be tokenized (numbers only)\"\"\"\n",
    "    sample = values.dropna().iloc[0] if not values.dropna().empty else None\n",
    "    if sample is None:\n",
    "        return False\n",
    "    \n",
    "    if isinstance(sample, str):\n",
    "        try:\n",
    "            parsed = parse_list_string(sample)\n",
    "            return parsed and all(isinstance(x, (int, np.integer)) or \n",
    "                               (isinstance(x, str) and x.isdigit()) for x in parsed)\n",
    "        except:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "# Build vocabulary mappings based on available data\n",
    "print(\"\\nBuilding vocabulary mappings from RAW to PP datasets...\")\n",
    "\n",
    "# Identify fields that need detokenization\n",
    "tokenized_fields = []\n",
    "for col in pp_df.columns:\n",
    "    if col in raw_df.columns and col in ['ingredients', 'steps', 'tags']:\n",
    "        if is_tokenized(pp_df[col]):\n",
    "            tokenized_fields.append(col)\n",
    "            print(f\"Field '{col}' appears to be tokenized and will be processed\")\n",
    "\n",
    "# Extract matching recipes to build vocabulary\n",
    "matched_recipes = []\n",
    "if common_ids:\n",
    "    # Use a sample of common IDs (up to 1000) to build vocabulary\n",
    "    sample_ids = list(common_ids)[:1000]\n",
    "    for id in sample_ids:\n",
    "        pp_idx = pp_dict[id]\n",
    "        raw_idx = raw_dict[id]\n",
    "        matched_recipes.append((pp_df.iloc[pp_idx], raw_df.iloc[raw_idx]))\n",
    "else:\n",
    "    # If no common IDs, use the first 1000 rows assuming alignment\n",
    "    for i in range(min(1000, len(pp_df), len(raw_df))):\n",
    "        matched_recipes.append((pp_df.iloc[i], raw_df.iloc[i]))\n",
    "\n",
    "# Build vocabulary mappings\n",
    "print(\"\\nBuilding detokenization mappings...\")\n",
    "vocabularies = {}\n",
    "\n",
    "for field in tokenized_fields:\n",
    "    token_to_text = {}\n",
    "    text_to_token = {}\n",
    "    \n",
    "    for pp_row, raw_row in matched_recipes:\n",
    "        # Skip if either field is missing\n",
    "        if pd.isna(pp_row[field]) or pd.isna(raw_row[field]):\n",
    "            continue\n",
    "            \n",
    "        pp_values = parse_list_string(pp_row[field])\n",
    "        raw_values = parse_list_string(raw_row[field])\n",
    "        \n",
    "        # Only map if the lists have the same length\n",
    "        if len(pp_values) == len(raw_values):\n",
    "            for token, text in zip(pp_values, raw_values):\n",
    "                token_str = str(token)\n",
    "                token_to_text[token_str] = text\n",
    "                text_to_token[text] = token_str\n",
    "    \n",
    "    vocabularies[field] = {\n",
    "        'token_to_text': token_to_text,\n",
    "        'text_to_token': text_to_token\n",
    "    }\n",
    "    print(f\"Built vocabulary for '{field}' with {len(token_to_text)} mappings\")\n",
    "\n",
    "# Function to detokenize using the built vocabulary\n",
    "def detokenize_field(token_list, vocab):\n",
    "    if pd.isna(token_list):\n",
    "        return []\n",
    "    \n",
    "    # Parse the token list if it's a string\n",
    "    if isinstance(token_list, str):\n",
    "        tokens = parse_list_string(token_list)\n",
    "    else:\n",
    "        tokens = token_list\n",
    "    \n",
    "    # Convert each token to its text equivalent\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        token_str = str(token)\n",
    "        if token_str in vocab['token_to_text']:\n",
    "            result.append(vocab['token_to_text'][token_str])\n",
    "        else:\n",
    "            result.append(f\"UNKNOWN_{token_str}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply detokenization to the PP dataset\n",
    "print(\"\\nApplying detokenization to the dataset...\")\n",
    "for field in tokenized_fields:\n",
    "    field_vocab = vocabularies[field]\n",
    "    pp_df[f'{field}_detokenized'] = pp_df[field].apply(\n",
    "        lambda x: detokenize_field(x, field_vocab)\n",
    "    )\n",
    "\n",
    "# Save vocabularies for future use\n",
    "print(\"\\nSaving vocabulary mappings...\")\n",
    "for field, vocab in vocabularies.items():\n",
    "    with open(f'{field}_vocabulary.json', 'w') as f:\n",
    "        json.dump(vocab, f, indent=2)\n",
    "    print(f\"Saved {field}_vocabulary.json\")\n",
    "\n",
    "# Save the detokenized dataset\n",
    "print(\"\\nSaving detokenized dataset...\")\n",
    "pp_df.to_csv(r'C:\\Users\\MehakGanju\\Documents\\Python Scripts\\SmartHeath POC\\kaggle recipe data\\detokenized_recipes.csv', index=False)\n",
    "print(\"Saved detokenized dataset to 'detokenized_recipes.csv'\")\n",
    "\n",
    "# Display sample of detokenized data\n",
    "print(\"\\nSample of detokenized data:\")\n",
    "sample_idx = 0\n",
    "if not pp_df.empty:\n",
    "    print(f\"Recipe: {pp_df.iloc[sample_idx].get('name', 'N/A')}\")\n",
    "    \n",
    "    for field in tokenized_fields:\n",
    "        detok_field = f'{field}_detokenized'\n",
    "        if detok_field in pp_df.columns:\n",
    "            values = pp_df.iloc[sample_idx][detok_field]\n",
    "            print(f\"\\n{field.capitalize()}:\")\n",
    "            for i, value in enumerate(values):\n",
    "                print(f\"  {i+1}. {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0161d15-82da-4759-90d9-00a6e2821a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(r'C:\\Users\\MehakGanju\\Documents\\Python Scripts\\SmartHeath POC\\kaggle recipe data\\detokenized_recipes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e1aa7be-a4c6-423f-834f-ee1627ea593c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>i</th>\n",
       "      <th>name_tokens</th>\n",
       "      <th>ingredient_tokens</th>\n",
       "      <th>steps_tokens</th>\n",
       "      <th>techniques</th>\n",
       "      <th>calorie_level</th>\n",
       "      <th>ingredient_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424415</td>\n",
       "      <td>23</td>\n",
       "      <td>[40480, 37229, 2911, 1019, 249, 6878, 6878, 28...</td>\n",
       "      <td>[[2911, 1019, 249, 6878], [1353], [6953], [153...</td>\n",
       "      <td>[40480, 40482, 21662, 481, 6878, 500, 246, 161...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[389, 7655, 6270, 1527, 3406]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146223</td>\n",
       "      <td>96900</td>\n",
       "      <td>[40480, 18376, 7056, 246, 1531, 2032, 40481]</td>\n",
       "      <td>[[17918], [25916], [2507, 6444], [8467, 1179],...</td>\n",
       "      <td>[40480, 40482, 729, 2525, 10906, 485, 43, 8393...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2683, 4969, 800, 5298, 840, 2499, 6632, 7022,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312329</td>\n",
       "      <td>120056</td>\n",
       "      <td>[40480, 21044, 16954, 8294, 556, 10837, 40481]</td>\n",
       "      <td>[[5867, 24176], [1353], [6953], [1301, 11332],...</td>\n",
       "      <td>[40480, 40482, 8240, 481, 24176, 296, 1353, 66...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1257, 7655, 6270, 590, 5024, 1119, 4883, 6696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74301</td>\n",
       "      <td>168258</td>\n",
       "      <td>[40480, 10025, 31156, 40481]</td>\n",
       "      <td>[[1270, 1645, 28447], [21601], [27952, 29471, ...</td>\n",
       "      <td>[40480, 40482, 5539, 21601, 1073, 903, 2324, 4...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[7940, 3609, 7060, 6265, 1170, 6654, 5003, 3561]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76272</td>\n",
       "      <td>109030</td>\n",
       "      <td>[40480, 17841, 252, 782, 2373, 1641, 2373, 252...</td>\n",
       "      <td>[[1430, 11434], [1430, 17027], [1615, 23, 695,...</td>\n",
       "      <td>[40480, 40482, 14046, 1430, 11434, 488, 17027,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[3484, 6324, 7594, 243]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       i                                        name_tokens  \\\n",
       "0  424415      23  [40480, 37229, 2911, 1019, 249, 6878, 6878, 28...   \n",
       "1  146223   96900       [40480, 18376, 7056, 246, 1531, 2032, 40481]   \n",
       "2  312329  120056     [40480, 21044, 16954, 8294, 556, 10837, 40481]   \n",
       "3   74301  168258                       [40480, 10025, 31156, 40481]   \n",
       "4   76272  109030  [40480, 17841, 252, 782, 2373, 1641, 2373, 252...   \n",
       "\n",
       "                                   ingredient_tokens  \\\n",
       "0  [[2911, 1019, 249, 6878], [1353], [6953], [153...   \n",
       "1  [[17918], [25916], [2507, 6444], [8467, 1179],...   \n",
       "2  [[5867, 24176], [1353], [6953], [1301, 11332],...   \n",
       "3  [[1270, 1645, 28447], [21601], [27952, 29471, ...   \n",
       "4  [[1430, 11434], [1430, 17027], [1615, 23, 695,...   \n",
       "\n",
       "                                        steps_tokens  \\\n",
       "0  [40480, 40482, 21662, 481, 6878, 500, 246, 161...   \n",
       "1  [40480, 40482, 729, 2525, 10906, 485, 43, 8393...   \n",
       "2  [40480, 40482, 8240, 481, 24176, 296, 1353, 66...   \n",
       "3  [40480, 40482, 5539, 21601, 1073, 903, 2324, 4...   \n",
       "4  [40480, 40482, 14046, 1430, 11434, 488, 17027,...   \n",
       "\n",
       "                                          techniques  calorie_level  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...              0   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...              0   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...              1   \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...              0   \n",
       "\n",
       "                                      ingredient_ids  \n",
       "0                      [389, 7655, 6270, 1527, 3406]  \n",
       "1  [2683, 4969, 800, 5298, 840, 2499, 6632, 7022,...  \n",
       "2  [1257, 7655, 6270, 590, 5024, 1119, 4883, 6696...  \n",
       "3   [7940, 3609, 7060, 6265, 1170, 6654, 5003, 3561]  \n",
       "4                            [3484, 6324, 7594, 243]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58e37be1-8b92-40af-bbe8-669468b5caa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'i',\n",
       " 'name_tokens',\n",
       " 'ingredient_tokens',\n",
       " 'steps_tokens',\n",
       " 'techniques',\n",
       " 'calorie_level',\n",
       " 'ingredient_ids']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95e0b1-377b-4b85-92e7-c27b83ea3258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
